{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaching using BERT to analyze the overview column plan:\n",
    "* Problem Understanding\n",
    "  * Extract themes, genres, or patterns that correlate with popularity\n",
    "  * BERT can reveal the underlying patterns by identifying contextual keywords and themes\n",
    "* Using BERT for Key Information Extraction\n",
    "  * BERT is a pre-trained model that reads text in both directions (both directions means that it reads starting from the left and the right side)\n",
    "  * This is good for understanding word context better\n",
    "  * Encoder vs. Decoder\n",
    "    * Encoder: Extracts contextual information (good for classification, clustering)\n",
    "      * Goal is to assign labels (genre, sentiment) based on the content OR Goal is to group similar texts together based on their meaning or themes\n",
    "      * BERT understands word meanings more since it reads both left and right contexts\n",
    "    * Decoder: Generates sequences (like summaries or paraphrasing)\n",
    "  * Encoder would be best since we can classify the theme of the show as \"romance,\" \"heist,\" etc.\n",
    "  * Decoder might be used if we need to summarize the overview or generate a more compact feature from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractions via Embeddings\n",
    "* Convert each overview into BERT embeddings, which is our vector representations\n",
    "* Use a pre-trained BERT model form Hugging Face (bert-base-uncased) to generate enbeddings\n",
    "  * BERT-base is the original configuration of the BERT model\n",
    "  * Using uncased model since the capitalization does not impact the meaning\n",
    "  * Will test the results of the BERT model on cleaned and uncleaned text data\n",
    "    * Read that using over cleaned data into the BERT model can negatively affect its performance\n",
    "  * Can use cosine similarity to show the similarity between overviews (clustering shows with similar themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "TMDB_filename = os.path.join(os.getcwd(), \"TMDB_tv_dataset_v3.csv\")\n",
    "df = pd.read_csv(TMDB_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers library, which is a popular open-source library from HUgging Face\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# import PyTorch to develop and train deep learning models\n",
    "import torch\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# loading the pre-trained BERT model and tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# tokenize and create tensor input\n",
    "def get_embedding(text): \n",
    "  if pd.isnull(text):\n",
    "    return torch.zeros(model.config.hidden_size).tolist()\n",
    "  '''\n",
    "  padding is set to true so that all input sequences have the same length\n",
    "  truncation is set to true so that it truncates longer sequences that are longer\n",
    "  than the models maximum input length so the text does not exceed the models capacity\n",
    "  'return_tensor=\"pt\"' means the output should be returned as a PyTorch tensor since the \n",
    "  model requires input in tensor format to perform computations\n",
    "  '''\n",
    "  tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "  '''\n",
    "  torch.no_grad() disables gradient tracking to reduce memory consumption for computations that do not require gradients\n",
    "  model(**tokens) passes the tokenized input to the pre-trained model (BERT)\n",
    "  '''\n",
    "  with torch.no_grad(): # generate embeddings \n",
    "    outputs = model(**tokens)\n",
    "  '''\n",
    "  outputs.last_hidden_state retrieves the hidden states from the last layer of the model for all tokens in the input sequence.\n",
    "  Each token has an associated embedding vector.\n",
    "  mean(dim=1) calculates the mean of the embeddings along the token dimension, which produces a single embedding vector for the entire input text, which\n",
    "  can be used for various downstream tasks like classification, clustering, etc.\n",
    "  '''\n",
    "  return outputs.last_hidden_state.mean(dim=1).squeeze().tolist() # average pooling of embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process the DataFrame with progress tracking\n",
    "def process_with_progress(series):\n",
    "  embeddings = []\n",
    "  for text in tqdm(series, desc=\"Processing Embeddings\"):\n",
    "    embeddings.append(get_embedding(text))\n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[(df['genres'].notna() & (df['genres'] != 'Unknown')) | (df['overview'].notna() & (df['overview'] != 'Unknown'))]\n",
    "\n",
    "missing_both = df[(df['genres'].isna() | (df['genres'] == 'Unknown')) & (df['overview'].isna() | (df['overview'] == 'Unknown'))]\n",
    "\n",
    "percentage_missing_both = (len(missing_both) / len(df)) * 100\n",
    "print(f\"Percentage of rows with both 'genres' and 'overview' missing: {percentage_missing_both:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the get_embedding function on the text data from the overview column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the overview column and assign embeddings to a new column\n",
    "df['bert_cleaned_overview'] = process_with_progress(df['overview'])\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows where both 'genres' and 'overview' are either NaN or 'Unknown'\n",
    "df_filtered = df[\n",
    "  ~((df['genres'].isna() | (df['genres'] == 'Unknown')) & (df['overview'].isna() | (df['overview'] == 'Unknown')))\n",
    "]\n",
    "print(f\"Filtered DataFrame shape: {df_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering/Classification\n",
    "* After converting the overviews to embeddings:\n",
    "  * Use clustering algorithms (K-Means) to find shows with similar themes\n",
    "  * Classification model to predict genre based on overview content\n",
    "  * If we noticed that certain clustered shows share a theme like \"heist\" we can make this a new feature in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess by ensuring the embeddings are in a suitable format (numpy array) and potentially normalize them\n",
    "embeddings_array = np.vstack(df_filtered['bert_cleaned_overview'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use MiniBatchKMeans since it is a memory-efficient and faster version of KMeans that avoids some threading issues so we can do the clustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "num_clusters = 5\n",
    "kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, batch_size=100)\n",
    "df_filtered['cluster'] = kmeans.fit_predict(embeddings_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since we are using BERT-base embeddings, each embedding vector has 768 dimensions\n",
    "* This means for every text in our dataset, there is a 768-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the shape to see the dimension of the data\n",
    "print(f\"Shape of embeddings array: {embeddings_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PCA to reduce dimensionality since the data is high-dimensional when visualizing\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2) # reduce dimensions to 2 for visualization\n",
    "reduced_embeddings = pca.fit_transform(embeddings_array)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=df_filtered['cluster'], cmap='viridis', alpha=0.5)\n",
    "plt.title('K-means Clustering of TV Show Overviews')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering results for TV show overviews reduced to two dimensions using PCA\n",
    "* Axes (PCA Component 1 and 2)\n",
    "  * The first two principle components resulting from PCA (PCA is used to reduce the dimensionality of the data)\n",
    "  * Since the BERT embedings were high-dimensional, PCA was applied to project them in a 2D space\n",
    "* Clusters\n",
    "  * Each point represent a TV shows overview text that was converted into a BERT embedding\n",
    "  * We specified 5 clusters, so the data points are grouped based on the similarity\n",
    "  * The similarity between tv shows is based on the semantic meaning captured by the BERT embeddings\n",
    "  * Shows that are closer have more similar content in their overviews, while shows in different clusters have distinct textual differences\n",
    "  * The color bar on the right hand side indicates the cluster label assigned by the K-means algorithm\n",
    "    * The numbers 0, 1, 2, 3, 4 correspond to different clusters\n",
    "* Insights\n",
    "  * The overlap in the clusters means that some TV shows share themes across multiple groups\n",
    "  * The isolated blue cluster on the right could suggest a group of TV shows with overviews that are semantically distinct from other shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "* Analyze the content of the clusters by looking at the shows in each group\n",
    "* Experiment with more clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Sample Data for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in range(num_clusters):\n",
    "  print(f\"\\nCluster {cluster_id}:\")\n",
    "  display(df_filtered[df_filtered['cluster'] == cluster_id][['name', 'overview', 'genres']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Genre Distributions per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# create a breakdown of genres in each cluster\n",
    "for cluster_id in range(num_clusters):\n",
    "    print(f\"\\nCluster {cluster_id} Genre Distribution:\")\n",
    "    genre_counts = df[df['cluster'] == cluster_id]['genres'].value_counts()\n",
    "    print(genre_counts.head(10))  # show top 10 most common genres\n",
    "'''\n",
    "\n",
    "# iterate over each cluster to display the top 10 genres for each cluster\n",
    "for cluster_id in range(num_clusters):\n",
    "\tprint(f\"\\nCluster {cluster_id} Genre Distribution:\")\n",
    "\tcluster_df = df_filtered[df_filtered['cluster'] == cluster_id]\n",
    "\tgenre_sums = cluster_df.loc[:, 'Action & Adventure':'Western'].sum()\n",
    "\tsorted_genres = genre_sums.sort_values(ascending=False)\n",
    "\tprint(sorted_genres.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Genre Distribution Per Cluster For Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# flatten genres into individual entries\n",
    "df_exploded = df.explode('genres')  # if 'genres' is a list\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(data=df_exploded, x='genres', hue='cluster')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Genre Distribution by Cluster\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify Themes or Topics in Overviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get the list of stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# function to get the top words from overviews in a given cluster\n",
    "def get_top_words(cluster_id, num_words=20):\n",
    "  overviews = df_filtered[df_filtered['cluster'] == cluster_id]['overview'].fillna('')\n",
    "  \n",
    "  # combine all overviews into a single string, and split into words\n",
    "  all_words = ' '.join(overviews).lower()\n",
    "  \n",
    "  # remove punctuation and split into words\n",
    "  all_words = re.findall(r'\\b\\w+\\b', all_words)\n",
    "  \n",
    "  # filter out stop words\n",
    "  filtered_words = [word for word in all_words if word not in stop_words]\n",
    "  \n",
    "  # count word frequencies\n",
    "  word_counts = Counter(filtered_words)\n",
    "  \n",
    "  return word_counts.most_common(num_words)\n",
    "\n",
    "# print the top words for each cluster\n",
    "for cluster_id in range(num_clusters):\n",
    "  print(f\"\\nTop words in Cluster {cluster_id}:\")\n",
    "  print(get_top_words(cluster_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Word Cloud for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def generate_word_cloud(cluster_id):\n",
    "  overviews = df_filtered[df_filtered['cluster'] == cluster_id]['overview'].fillna('')\n",
    "  all_words = ' '.join(overviews).lower()\n",
    "  words = re.findall(r'\\b\\w+\\b', all_words)\n",
    "  filtered_words = [word for word in words if word not in stop_words]\n",
    "  text = ' '.join(filtered_words)\n",
    "\n",
    "  # create the word cloud object\n",
    "  wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "  \n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.imshow(wordcloud, interpolation='bilinear')\n",
    "  plt.axis('off')\n",
    "  plt.title(f'Word Cloud for Cluster {cluster_id}', fontsize=16)\n",
    "  plt.show()\n",
    "\n",
    "# generate word clouds for all clusters\n",
    "for cluster_id in range(num_clusters):\n",
    "  generate_word_cloud(cluster_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_top_words = {}\n",
    "for cluster_id in range(num_clusters):\n",
    "  cluster_top_words[cluster_id] = get_top_words(cluster_id)\n",
    "\n",
    "# plot the top words of each cluster\n",
    "for cluster_id in range(num_clusters):\n",
    "\t# get top words and their counts for the current cluster\n",
    "\twords, counts = zip(*get_top_words(cluster_id))\n",
    "\t\n",
    "\tplt.figure(figsize=(10, 4))\n",
    "\tplt.bar(words, counts, color='skyblue')\n",
    "\tplt.title(f\"Top Words in Cluster {cluster_id}\", fontsize=14)\n",
    "\tplt.xlabel(\"Words\", fontsize=12)\n",
    "\tplt.ylabel(\"Frequency\", fontsize=12)\n",
    "\tplt.xticks(rotation=45, fontsize=10)\n",
    "\tplt.tight_layout() \n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# filter rows where the word \"family\" appears in the 'overview' column\n",
    "family_shows = df_filtered[df_filtered['cleaned_overview'].str.contains(r'\\bfamily\\b', case=False, na=False)]\n",
    "\n",
    "html_output = family_shows[['name', 'popularity']].to_html()\n",
    "display(HTML(f'<div style=\"max-height: 300px; overflow-y: scroll;\">{html_output}</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature 'has_family' that is 1 if 'family' is in the overview, else 0\n",
    "df['has_family'] = df['cleaned_overview'].fillna('').str.contains(r'\\bfamily\\b', case=False, na=False).astype(int)\n",
    "\n",
    "print(df[['cleaned_overview', 'has_family']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['has_family'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good seperation of clusters\n",
    "# Popularities for each cluster\n",
    "# Getting a tag (boolean fld ex: including the word 'world')\n",
    "# If a word is very popular, or two words are corelated ot eachother, find relations between words\n",
    "# create a feature based on the word (can be multiple words)\n",
    "# family an dlove can be a feature\n",
    "# because we are missing 44% of the data, we should combine the overview column with another feature\n",
    "# if we are able to get a genre for that, we can consider the other 44 percent, so we can populate the missing genre\n",
    "# Witht he overview column, we can consider how the model performs with the overview column and without it\n",
    "# If it is not making much of a difference, we can exclude it\n",
    "# Simple model to predict the existing popularity\n",
    "\n",
    "# llamma model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_popularity_per_cluster = df_filtered.groupby('cluster')['popularity'].mean()\n",
    "\n",
    "print(\"Average Popularity per Cluster:\")\n",
    "print(average_popularity_per_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot to visualize average popularity for each cluster\n",
    "average_popularity_per_cluster.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Average Popularity')\n",
    "plt.title('Average Popularity Across Clusters')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# do some log transformation\n",
    "# check how many rows have a missing overview but has a genre and vice versa\n",
    "# If both are missing we can ignore for now and see how the BERT model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df_filtered['popularity'].corr(df_filtered['cluster'])\n",
    "print(f\"Correlation between Popularity and Clusters: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing or 'Unknown' overview but a valid genre\n",
    "missing_or_unknown_overview_with_genre = df[\n",
    "  (df['overview'].isna() | (df['overview'] == 'Unknown')) & \n",
    "  ~(df['genres'].isna() | (df['genres'] == 'Unknown'))\n",
    "]\n",
    "count_missing_or_unknown_overview_with_genre = missing_or_unknown_overview_with_genre.shape[0]\n",
    "\n",
    "# missing or 'Unknown' genre but a valid overview\n",
    "missing_or_unknown_genre_with_overview = df[\n",
    "  (df['genres'].isna() | (df['genres'] == 'Unknown')) & \n",
    "  ~(df['overview'].isna() | (df['overview'] == 'Unknown'))\n",
    "]\n",
    "count_missing_or_unknown_genre_with_overview = missing_or_unknown_genre_with_overview.shape[0]\n",
    "\n",
    "print(f\"Number of rows with missing or 'Unknown' overview but a valid genre: {count_missing_or_unknown_overview_with_genre}\")\n",
    "print(f\"Number of rows with missing or 'Unknown' genre but a valid overview: {count_missing_or_unknown_genre_with_overview}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to a CSV file without the BERT embeddings\n",
    "# column_to_exclude = 'bert_cleaned_overview'\n",
    "# df.drop(columns=[column_to_exclude]).to_csv(\"TMDB_tv_dataset_v3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
