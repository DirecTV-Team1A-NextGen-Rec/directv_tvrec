{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaching using BERT to analyze the overview column plan:\n",
    "* Problem Understanding\n",
    "  * Extract themes, genres, or patterns that correlate with popularity\n",
    "  * BERT can reveal the underlying patterns by identifying contextual keywords and themes\n",
    "* Using BERT for Key Information Extraction\n",
    "  * BERT is a pre-trained model that reads text in both directions (both directions means that it reads starting from the left and the right side)\n",
    "  * This is good for understanding word context better\n",
    "  * Encoder vs. Decoder\n",
    "    * Encoder: Extracts contextual information (good for classification, clustering)\n",
    "      * Goal is to assign labels (genre, sentiment) based on the content OR Goal is to group similar texts together based on their meaning or themes\n",
    "      * BERT understands word meanings more since it reads both left and right contexts\n",
    "    * Decoder: Generates sequences (like summaries or paraphrasing)\n",
    "  * Encoder would be best since we can classify the theme of the show as \"romance,\" \"heist,\" etc.\n",
    "  * Decoder might be used if we need to summarize the overview or generate a more compact feature from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractions via Embeddings\n",
    "* Convert each overview into BERT embeddings, which is our vector representations\n",
    "* Use a pre-trained BERT model form Hugging Face (bert-base-uncased) to generate enbeddings\n",
    "  * BERT-base is the original configuration of the BERT model\n",
    "  * Using uncased model since the capitalization does not impact the meaning\n",
    "  * Will test the results of the BERT model on cleaned and uncleaned text data\n",
    "    * Read that using over cleaned data into the BERT model can negatively affect its performance\n",
    "  * Can use cosine similarity to show the similarity between overviews (clustering shows with similar themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "TMDB_filename = os.path.join(os.getcwd(), \"TMDB_tv_dataset_v3.csv\")\n",
    "df = pd.read_csv(TMDB_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers library, which is a popular open-source library from HUgging Face\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# import PyTorch to develop and train deep learning models\n",
    "import torch\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# loading the pre-trained BERT model and tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# tokenize and create tensor input\n",
    "def get_embedding(text): \n",
    "  if pd.isnull(text):\n",
    "    return torch.zeros(model.config.hidden_size).tolist()\n",
    "  '''\n",
    "  padding is set to true so that all input sequences have the same length\n",
    "  truncation is set to true so that it truncates longer sequences that are longer\n",
    "  than the models maximum input length so the text does not exceed the models capacity\n",
    "  'return_tensor=\"pt\"' means the output should be returned as a PyTorch tensor since the \n",
    "  model requires input in tensor format to perform computations\n",
    "  '''\n",
    "  tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "  '''\n",
    "  torch.no_grad() disables gradient tracking to reduce memory consumption for computations that do not require gradients\n",
    "  model(**tokens) passes the tokenized input to the pre-trained model (BERT)\n",
    "  '''\n",
    "  with torch.no_grad(): # generate embeddings \n",
    "    outputs = model(**tokens)\n",
    "  '''\n",
    "  outputs.last_hidden_state retrieves the hidden states from the last layer of the model for all tokens in the input sequence.\n",
    "  Each token has an associated embedding vector.\n",
    "  mean(dim=1) calculates the mean of the embeddings along the token dimension, which produces a single embedding vector for the entire input text, which\n",
    "  can be used for various downstream tasks like classification, clustering, etc.\n",
    "  '''\n",
    "  return outputs.last_hidden_state.mean(dim=1).squeeze().tolist() # average pooling of embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process the DataFrame with progress tracking\n",
    "def process_with_progress(series):\n",
    "  embeddings = []\n",
    "  for text in tqdm(series, desc=\"Processing Embeddings\"):\n",
    "    embeddings.append(get_embedding(text))\n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the get_embedding function on the text data from the overview column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the overview column and assign embeddings to a new column\n",
    "df['bert_cleaned_overview'] = process_with_progress(df['overview'])\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering/Classification\n",
    "* After converting the overviews to embeddings:\n",
    "  * Use clustering algorithms (K-Means) to find shows with similar themes\n",
    "  * Classification model to predict genre based on overview content\n",
    "  * If we noticed that certain clustered shows share a theme like \"heist\" we can make this a new feature in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess by ensuring the embeddings are in a suitable format (numpy array) and potentially normalize them\n",
    "embeddings_array = np.vstack(df['bert_cleaned_overview'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use MiniBatchKMeans since it is a memory-efficient and faster version of KMeans that avoids some threading issues so we can do the clustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# \n",
    "num_clusters = 5\n",
    "kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=0, batch_size=100)\n",
    "df['cluster'] = kmeans.fit_predict(embeddings_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since we are using BERT-base embeddings, each embedding vector has 768 dimensions\n",
    "* This means for every text in our dataset, there is a 768-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the shape to see the dimension of the data\n",
    "print(f\"Shape of embeddings array: {embeddings_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PCA to reduce dimensionality since the data is high-dimensional when visualizing\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2) # reduce dimensions to 2 for visualization\n",
    "reduced_embeddings = pca.fit_transform(embeddings_array)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=df['cluster'], cmap='viridis', alpha=0.5)\n",
    "plt.title('K-means Clustering of TV Show Overviews')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering results for TV show overviews reduced to two dimensions using PCA\n",
    "* Axes (PCA Component 1 and 2)\n",
    "  * The first two principle components resulting from PCA (PCA is used to reduce the dimensionality of the data)\n",
    "  * Since the BERT embedings were high-dimensional, PCA was applied to project them in a 2D space\n",
    "* Clusters\n",
    "  * Each point represent a TV shows overview text that was converted into a BERT embedding\n",
    "  * We specified 5 clusters, so the data points are grouped based on the similarity\n",
    "  * The similarity between tv shows is based on the semantic meaning captured by the BERT embeddings\n",
    "  * Shows that are closer have more similar content in their overviews, while shows in different clusters have distinct textual differences\n",
    "  * The color bar on the right hand side indicates the cluster label assigned by the K-means algorithm\n",
    "    * The numbers 0, 1, 2, 3, 4 correspond to different clusters\n",
    "* Insights\n",
    "  * The overlap in the clusters means that some TV shows share themes across multiple groups\n",
    "  * The isolated blue cluster on the right could suggest a group of TV shows with overviews that are semantically distinct from other shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "* Analyze the content of the clusters by looking at the shows in each group\n",
    "* Experiment with more clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Sample Data for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in range(num_clusters):\n",
    "  print(f\"\\nCluster {cluster_id}:\")\n",
    "  display(df[df['cluster'] == cluster_id][['name', 'overview', 'genres']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Genre Distributions per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the one-hot encoded genre data\n",
    "cleaned_TMDB_filename = os.path.join(os.getcwd(), \"TMDB_tv_dataset_v3.csv\")\n",
    "df_onehot = pd.read_csv(cleaned_TMDB_filename)\n",
    "\n",
    "# Check for duplicates or missing values in indexes\n",
    "print(df.index.duplicated().sum())  # Should be zero if no duplicates\n",
    "print(df_onehot.index.duplicated().sum())  # Same here\n",
    "print(len(set(df.index) - set(df_onehot.index)))  # Shows any missing values\n",
    "\n",
    "# Ensure the first column is the index and remove duplicates if any\n",
    "df_onehot = df_onehot.set_index(df_onehot.columns[0], drop=True).loc[~df_onehot.index.duplicated(keep='first')]\n",
    "\n",
    "# Load the cluster dataset and reset the index if needed\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# List of genre columns\n",
    "genre_columns = [\n",
    "  'Action & Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary', \n",
    "  'Drama', 'Family', 'History', 'Kids', 'Music', 'Musical', 'Mystery', \n",
    "  'News', 'Reality', 'Romance', 'Sci-Fi & Fantasy', 'Soap', 'Talk', \n",
    "  'War & Politics', 'Western'\n",
    "]\n",
    "\n",
    "# Add a column to one-hot encoded dataframe with cluster assignments\n",
    "df_onehot['cluster'] = df['cluster'].values\n",
    "\n",
    "# Group by the cluster column and sum up each genre column\n",
    "for cluster_id in range(df['cluster'].nunique()):\n",
    "  print(f\"\\nCluster {cluster_id} Genre Distribution:\")\n",
    "  # Get rows for the current cluster and sum genres\n",
    "  cluster_genre_counts = df_onehot[df_onehot['cluster'] == cluster_id][genre_columns].sum()\n",
    "  print(cluster_genre_counts.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Genre Distribution Per Cluster For Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten genres into individual entries\n",
    "df_exploded = df.explode('genres')  # if 'genres' is a list\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(data=df_exploded, x='genres', hue='cluster')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Genre Distribution by Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify Themes or Topics in Overviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get the list of stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# function to get the top words from overviews in a given cluster\n",
    "def get_top_words(cluster_id, num_words=20):\n",
    "  overviews = df[df['cluster'] == cluster_id]['overview'].fillna('')\n",
    "  \n",
    "  # combine all overviews into a single string, and split into words\n",
    "  all_words = ' '.join(overviews).lower()\n",
    "  \n",
    "  # remove punctuation and split into words\n",
    "  all_words = re.findall(r'\\b\\w+\\b', all_words)\n",
    "  \n",
    "  # filter out stop words\n",
    "  filtered_words = [word for word in all_words if word not in stop_words]\n",
    "  \n",
    "  # count word frequencies\n",
    "  word_counts = Counter(filtered_words)\n",
    "  \n",
    "  return word_counts.most_common(num_words)\n",
    "\n",
    "# print the top words for each cluster\n",
    "for cluster_id in range(num_clusters):\n",
    "  print(f\"\\nTop words in Cluster {cluster_id}:\")\n",
    "  print(get_top_words(cluster_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Word Cloud for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def generate_word_cloud(cluster_id):\n",
    "  overviews = df[df['cluster'] == cluster_id]['overview'].fillna('')\n",
    "  all_words = ' '.join(overviews).lower()\n",
    "  words = re.findall(r'\\b\\w+\\b', all_words)\n",
    "  filtered_words = [word for word in words if word not in stop_words]\n",
    "  text = ' '.join(filtered_words)\n",
    "\n",
    "  # create the word cloud object\n",
    "  wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "  \n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.imshow(wordcloud, interpolation='bilinear')\n",
    "  plt.axis('off')\n",
    "  plt.title(f'Word Cloud for Cluster {cluster_id}', fontsize=16)\n",
    "  plt.show()\n",
    "\n",
    "# generate word clouds for all clusters\n",
    "for cluster_id in range(num_clusters):\n",
    "  generate_word_cloud(cluster_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good seperation of clusters\n",
    "# Popularities for each cluster\n",
    "# Getting a tag (boolean fld ex: including the word 'world')\n",
    "# If a word is very popular, or two words are corelated ot eachother, find relations between words\n",
    "# create a feature based on the word (can be multiple words)\n",
    "# family an dlove can be a feature\n",
    "# because we are missing 44% of the data, we should combine the overview column with another feature\n",
    "# if we are able to get a genre for that, we can consider the other 44 percent, so we can populate the missing genre\n",
    "# Witht he overview column, we can consider how the model performs with the overview column and without it\n",
    "# If it is not making much of a difference, we can exclude it\n",
    "# Simple model to predict the existing popularity\n",
    "\n",
    "# llamma model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
