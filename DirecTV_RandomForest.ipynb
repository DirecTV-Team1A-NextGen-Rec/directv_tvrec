{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMDB_filename = os.path.join(os.getcwd(), \"TMDB_tv_dataset_v3.csv\")\n",
    "df = pd.read_csv(TMDB_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "columns_to_fill = [col for col in categorical_cols if col != 'genres']\n",
    "df[columns_to_fill] = df[columns_to_fill].apply(lambda col: col.fillna(col.mode()[0]))\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#One Hote Encoding\n",
    "df['genres'] = df['genres'].fillna('Unknown') # for genres that are empty just call them Unknown\n",
    "\n",
    "df['genres'] = df['genres'].apply(lambda x: x.split(', '))\n",
    "\n",
    "unique_genres = sorted(set(genre for genres in df['genres'] for genre in genres))\n",
    "\n",
    "for genre in unique_genres:\n",
    "  df[genre] = df['genres'].apply(lambda genres: int(genre in genres))\n",
    "\n",
    "df = df.drop('genres', axis=1)\n",
    "\n",
    "df.to_csv('TMDB_tv_dataset_v3.csv', index=False)\n",
    "\n",
    "list = df.select_dtypes(include=['object']).columns.tolist()\n",
    "encode_cols =['original_language','type', 'status','created_by','networks','origin_country','production_companies']\n",
    "\n",
    "# Creating one-hot encoded columns for original_language\n",
    "top_20_original_language = df['original_language'].value_counts(ascending = False).head(20).index.tolist()\n",
    "for i in top_20_original_language:\n",
    "    name = 'original-language_' + i\n",
    "    df[name] = np.where(df['original_language'] == i, 1, 0)\n",
    "\n",
    "# Removing the original_language column from df and encode_cols\n",
    "df.drop(columns = ['original_language'], inplace = True)\n",
    "encode_cols.remove('original_language')\n",
    "df.columns\n",
    "\n",
    "# Creating a list of the top 10 values in the created_by column\n",
    "top_10_created_by = df['created_by'].value_counts(ascending = False).head(10).index.tolist()\n",
    "# Creating one-hot encoded columns for created_by\n",
    "for i in top_10_created_by:\n",
    "    name = 'created-by_' + i\n",
    "    df[name] = np.where(df['created_by'] == i, 1, 0)\n",
    "\n",
    "# Removing the created_by column from df and encode_cols\n",
    "df.drop(columns = ['created_by'], inplace = True)\n",
    "encode_cols.remove('created_by')\n",
    "\n",
    "# Creating a list of the top 11 values in the networks column\n",
    "top_11_networks = df['networks'].value_counts(ascending = False).head(11).index.tolist()\n",
    "\n",
    "# Creating one-hot encoded columns for networks\n",
    "for i in top_11_networks:\n",
    "    name = 'networks_' + i\n",
    "    df[name] = np.where(df['networks'] == i, 1, 0)\n",
    "\n",
    "# Removing the networks column from df and encode_cols\n",
    "df.drop(columns = ['networks'], inplace = True)\n",
    "encode_cols.remove('networks')\n",
    "\n",
    "# Creating a list of the top 10 values in the origin_country column\n",
    "top_26_origin_country = df['origin_country'].value_counts(ascending = False).head(26).index.tolist()\n",
    "\n",
    "one_hot_encoded_origin_country = pd.DataFrame() # start with an empty dataframe\n",
    "\n",
    "for i in top_26_origin_country:\n",
    "    one_hot_encoded_origin_country['origin-country_' + i] = np.where(df['origin_country'] == i, 1, 0)\n",
    "\n",
    "# the we can concatenate the one-hot encoded columns to the original DataFrame\n",
    "df = pd.concat([df, one_hot_encoded_origin_country], axis=1)\n",
    "\n",
    "# Removing the origin_country column from df and encode_cols\n",
    "df.drop(columns = ['origin_country'], inplace = True)\n",
    "encode_cols.remove('origin_country')\n",
    "\n",
    "# Creating a list of the top 10 values in the production_companies column\n",
    "top_10_production_companies = df['production_companies'].value_counts(ascending = False).head(10).index.tolist()\n",
    "\n",
    "one_hot_encoded_production_companies = pd.DataFrame() # start with an empty dataframe\n",
    "\n",
    "for i in top_26_origin_country:\n",
    "    one_hot_encoded_production_companies['production-companies_' + i] = np.where(df['production_companies'] == i, 1, 0)\n",
    "\n",
    "# the we can concatenate the one-hot encoded columns to the original DataFrame\n",
    "df = pd.concat([df, one_hot_encoded_production_companies], axis=1)\n",
    "\n",
    "# Removing the production_companies column from df and encode_cols\n",
    "df.drop(columns = ['production_companies'], inplace = True)\n",
    "encode_cols.remove('production_companies')\n",
    "\n",
    "# One-hot encoding remaining columns\n",
    "for colname in encode_cols:\n",
    "    df_encoded = pd.get_dummies(df[colname], prefix=colname+'')\n",
    "    df = df.join(df_encoded)\n",
    "\n",
    "# Removing remaining original cols from df\n",
    "df.drop(columns=encode_cols,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n",
      "datetime64[ns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          SPRING\n",
       "1          WINTER\n",
       "2          SUMMER\n",
       "3            FALL\n",
       "4            FALL\n",
       "           ...   \n",
       "168412    UNKNOWN\n",
       "168416    UNKNOWN\n",
       "168418    UNKNOWN\n",
       "168419    UNKNOWN\n",
       "168420    UNKNOWN\n",
       "Name: last_air_date_season, Length: 168593, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting first_air_date and last_air_date columns to DateTime referring to https://www.youtube.com/watch?v=f7LODKIjtaA\n",
    "df['first_air_date'] = pd.to_datetime(df['first_air_date'], format = '%Y-%m-%d')\n",
    "df['last_air_date'] = pd.to_datetime(df['last_air_date'], format = '%Y-%m-%d' )\n",
    "print(df['first_air_date'].dtypes)\n",
    "print(df['last_air_date'].dtypes)\n",
    "\n",
    "# Creating a function to create seasons for each month\n",
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'WINTER'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'SPRING'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'SUMMER'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'FALL'\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "# Creating a column with the seasons for first_air_date\n",
    "df['first_air_date_season'] = df['first_air_date'].apply(get_season)\n",
    "df['first_air_date_season']\n",
    "\n",
    "# Creating columns for first_air_date_season as boolean values\n",
    "df['first_air_date_winter'] = df['first_air_date_season'] == 'WINTER'\n",
    "df['first_air_date_spring'] = df['first_air_date_season'] == 'SPRING'\n",
    "df['first_air_date_summer'] = df['first_air_date_season'] == 'SUMMER'\n",
    "df['first_air_date_fall'] = df['first_air_date_season'] == 'FALL'\n",
    "\n",
    "# Repeating same process for last_air_date\n",
    "df['last_air_date_season'] = df['last_air_date'].apply(get_season)\n",
    "df['last_air_date_season']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id              name  number_of_seasons  number_of_episodes  \\\n",
      "0   1399.0   Game of Thrones                8.0                73.0   \n",
      "1  71446.0       Money Heist                3.0                41.0   \n",
      "2  66732.0   Stranger Things                4.0                34.0   \n",
      "3   1402.0  The Walking Dead               11.0               177.0   \n",
      "4  63174.0           Lucifer                6.0                93.0   \n",
      "\n",
      "   vote_count  vote_average  \\\n",
      "0     21857.0         8.442   \n",
      "1     17836.0         8.257   \n",
      "2     16161.0         8.624   \n",
      "3     15432.0         8.121   \n",
      "4     13870.0         8.486   \n",
      "\n",
      "                                            overview  adult  \\\n",
      "0  Seven noble families fight for control of the ...  False   \n",
      "1  To carry out the biggest heist in history, a m...  False   \n",
      "2  When a young boy vanishes, a small town uncove...  False   \n",
      "3  Sheriff's deputy Rick Grimes awakens from a co...  False   \n",
      "4  Bored and unhappy as the Lord of Hell, Lucifer...  False   \n",
      "\n",
      "                      backdrop_path first_air_date  ... first_air_date_spring  \\\n",
      "0   /2OMB0ynKlyIenMJWI2Dy9IWT4c.jpg     2011-04-17  ...                  True   \n",
      "1  /gFZriCkpJYsApPZEF3jhxL4yLzG.jpg     2017-05-02  ...                  True   \n",
      "2  /2MaumbgBlW1NoPo3ZJO38A6v7OS.jpg     2016-07-15  ...                 False   \n",
      "3  /x4salpjB11umlUOltfNvSSrjSXm.jpg     2010-10-31  ...                 False   \n",
      "4  /aDBRtunw49UF4XmqfyNuD9nlYIu.jpg     2016-01-25  ...                 False   \n",
      "\n",
      "  first_air_date_summer first_air_date_fall last_air_date_season  \\\n",
      "0                 False               False               SPRING   \n",
      "1                 False               False               WINTER   \n",
      "2                  True               False               SUMMER   \n",
      "3                 False                True                 FALL   \n",
      "4                 False               False                 FALL   \n",
      "\n",
      "   number_of_seasons_log number_of_episodes_log vote_count_log  \\\n",
      "0               2.197225               4.304065       9.992322   \n",
      "1               1.386294               3.737670       9.789030   \n",
      "2               1.609438               3.555348       9.690418   \n",
      "3               2.484907               5.181784       9.644263   \n",
      "4               1.945910               4.543295       9.537556   \n",
      "\n",
      "  vote_average_log popularity_log episode_run_time_log  \n",
      "0         2.245168       6.989259             0.000000  \n",
      "1         2.225380       4.578354             4.262680  \n",
      "2         2.264260       5.229562             0.000000  \n",
      "3         2.210579       6.195927             3.761200  \n",
      "4         2.249817       6.034687             3.828641  \n",
      "\n",
      "[5 rows x 160 columns]\n",
      "       number_of_seasons_log  number_of_episodes_log  vote_count_log  \\\n",
      "count          167059.000000           167059.000000   167059.000000   \n",
      "mean                0.756984                2.014484        0.575712   \n",
      "std                 0.495780                1.432871        1.122633   \n",
      "min                 0.000000                0.000000        0.000000   \n",
      "25%                 0.693147                0.693147        0.000000   \n",
      "50%                 0.693147                1.945910        0.000000   \n",
      "75%                 0.693147                3.044522        0.693147   \n",
      "max                 5.484797                9.944629        9.992322   \n",
      "\n",
      "       vote_average_log  popularity_log  episode_run_time_log  \n",
      "count     167059.000000   167059.000000         167059.000000  \n",
      "mean           0.692656        1.030757              1.810148  \n",
      "std            0.976512        0.895498              1.872455  \n",
      "min            0.000000        0.000000              0.000000  \n",
      "25%            0.000000        0.470004              0.000000  \n",
      "50%            0.000000        0.628075              0.000000  \n",
      "75%            1.945910        1.243578              3.761200  \n",
      "max            2.397895        8.218250              8.705000  \n"
     ]
    }
   ],
   "source": [
    "# List of original numerical columns\n",
    "numerical_columns = ['number_of_seasons', 'number_of_episodes', 'vote_count', 'vote_average', 'popularity', 'episode_run_time']\n",
    "\n",
    "# Apply log transformation and rename columns with \"_log\"\n",
    "df_log = df[numerical_columns].apply(lambda x: np.log1p(x)).add_suffix('_log')\n",
    "\n",
    "# Concatenate the log-transformed columns with the original DataFrame\n",
    "df = pd.concat([df, df_log], axis=1)\n",
    "\n",
    "# Display the DataFrame to confirm new columns\n",
    "print(df.head())\n",
    "\n",
    "# List of log-transformed numerical columns\n",
    "numerical_columns_log = [col + '_log' for col in numerical_columns]\n",
    "\n",
    "# Display summary statistics for log-transformed columns\n",
    "print(df[numerical_columns_log].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Define the limits for Winsorization\n",
    "lower_limit = 0.05  # 5th percentile\n",
    "upper_limit = 0.95  # 95th percentile\n",
    "\n",
    "# Winsorize each numerical column and add it back to the DataFrame\n",
    "for col in numerical_columns:\n",
    "    df[col + '_winsorized'] = winsorize(df[col], limits=(lower_limit, 1 - upper_limit))\n",
    "\n",
    "# Winsorize each log-transformed numerical column and add it back to the DataFrame\n",
    "for col in numerical_columns_log:\n",
    "    df[col + '_winsorized'] = winsorize(df[col], limits=(lower_limit, 1 - upper_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id              name  number_of_seasons  number_of_episodes  \\\n",
      "0   1399.0   Game of Thrones                5.0                73.0   \n",
      "1  71446.0       Money Heist                3.0                41.0   \n",
      "2  66732.0   Stranger Things                4.0                34.0   \n",
      "3   1402.0  The Walking Dead                5.0               105.0   \n",
      "4  63174.0           Lucifer                5.0                93.0   \n",
      "\n",
      "   vote_count  vote_average  \\\n",
      "0        25.0         8.442   \n",
      "1        25.0         8.257   \n",
      "2        25.0         8.624   \n",
      "3        25.0         8.121   \n",
      "4        25.0         8.486   \n",
      "\n",
      "                                            overview  adult  \\\n",
      "0  Seven noble families fight for control of the ...  False   \n",
      "1  To carry out the biggest heist in history, a m...  False   \n",
      "2  When a young boy vanishes, a small town uncove...  False   \n",
      "3  Sheriff's deputy Rick Grimes awakens from a co...  False   \n",
      "4  Bored and unhappy as the Lord of Hell, Lucifer...  False   \n",
      "\n",
      "                      backdrop_path first_air_date  ...  \\\n",
      "0   /2OMB0ynKlyIenMJWI2Dy9IWT4c.jpg     2011-04-17  ...   \n",
      "1  /gFZriCkpJYsApPZEF3jhxL4yLzG.jpg     2017-05-02  ...   \n",
      "2  /2MaumbgBlW1NoPo3ZJO38A6v7OS.jpg     2016-07-15  ...   \n",
      "3  /x4salpjB11umlUOltfNvSSrjSXm.jpg     2010-10-31  ...   \n",
      "4  /aDBRtunw49UF4XmqfyNuD9nlYIu.jpg     2016-01-25  ...   \n",
      "\n",
      "  vote_count_winsorized_norm vote_average_winsorized_norm  \\\n",
      "0                        1.0                     0.938000   \n",
      "1                        1.0                     0.917444   \n",
      "2                        1.0                     0.958222   \n",
      "3                        1.0                     0.902333   \n",
      "4                        1.0                     0.942889   \n",
      "\n",
      "  popularity_winsorized_norm episode_run_time_winsorized_norm  \\\n",
      "0                        1.0                         0.000000   \n",
      "1                        1.0                         1.000000   \n",
      "2                        1.0                         0.000000   \n",
      "3                        1.0                         0.600000   \n",
      "4                        1.0                         0.642857   \n",
      "\n",
      "   number_of_seasons_log_winsorized_norm  \\\n",
      "0                               1.000000   \n",
      "1                               0.773706   \n",
      "2                               0.898244   \n",
      "3                               1.000000   \n",
      "4                               1.000000   \n",
      "\n",
      "  number_of_episodes_log_winsorized_norm vote_count_log_winsorized_norm  \\\n",
      "0                               0.922938                            1.0   \n",
      "1                               0.801484                            1.0   \n",
      "2                               0.762388                            1.0   \n",
      "3                               1.000000                            1.0   \n",
      "4                               0.974237                            1.0   \n",
      "\n",
      "  vote_average_log_winsorized_norm popularity_log_winsorized_norm  \\\n",
      "0                         0.975064                            1.0   \n",
      "1                         0.966470                            1.0   \n",
      "2                         0.983356                            1.0   \n",
      "3                         0.960042                            1.0   \n",
      "4                         0.977083                            1.0   \n",
      "\n",
      "  episode_run_time_log_winsorized_norm  \n",
      "0                             0.000000  \n",
      "1                             1.000000  \n",
      "2                             0.000000  \n",
      "3                             0.882356  \n",
      "4                             0.898177  \n",
      "\n",
      "[5 rows x 311 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
      "/var/folders/n4/c3jxqdvx2l15c7ghrhjmd95m0000gn/T/ipykernel_69927/3659227612.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select numerical columns\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply scaling and add columns with '_norm' suffix to the DataFrame\n",
    "df[[col + '_norm' for col in numeric_columns]] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Display the updated DataFrame to confirm new normalized columns\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with regular dataset, 4 numerical features:\n",
      "Train MAE: 1.0953\n",
      "Train MSE: 5.4330\n",
      "Train R^2: 0.8242\n",
      "Test MAE: 1.7127\n",
      "Test MSE: 12.3445\n",
      "Test R^2: 0.5991\n"
     ]
    }
   ],
   "source": [
    "X = df[['number_of_seasons', 'number_of_episodes', 'vote_count', 'vote_average', 'episode_run_time']]\n",
    "y = df['popularity']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test data\n",
    "rf_predictions_test = rf.predict(X_test)\n",
    "\n",
    "# Predictions on train data\n",
    "rf_predictions_train = rf.predict(X_train)\n",
    "\n",
    "# Calculate evaluation metrics for test set\n",
    "test_mae = mean_absolute_error(y_test, rf_predictions_test)\n",
    "test_mse = mean_squared_error(y_test, rf_predictions_test)\n",
    "test_r2 = r2_score(y_test, rf_predictions_test)\n",
    "\n",
    "# Calculate evaluation metrics for train set\n",
    "train_mae = mean_absolute_error(y_train, rf_predictions_train)\n",
    "train_mse = mean_squared_error(y_train, rf_predictions_train)\n",
    "train_r2 = r2_score(y_train, rf_predictions_train)\n",
    "\n",
    "print(\"Results with regular dataset, 4 numerical features:\")\n",
    "# Print Train set evaluation metrics\n",
    "print(f'Train MAE: {train_mae:.4f}')\n",
    "print(f'Train MSE: {train_mse:.4f}')\n",
    "print(f'Train R^2: {train_r2:.4f}')\n",
    "\n",
    "# Print Test set evaluation metrics\n",
    "print(f'Test MAE: {test_mae:.4f}')\n",
    "print(f'Test MSE: {test_mse:.4f}')\n",
    "print(f'Test R^2: {test_r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with Log Transformation, 4 features:\n",
      "Train MAE: 0.2227\n",
      "Train MSE: 0.1292\n",
      "Train R^2: 0.7903\n",
      "Test MAE: 0.3047\n",
      "Test MSE: 0.2261\n",
      "Test R^2: 0.6313\n"
     ]
    }
   ],
   "source": [
    "X = df[['number_of_seasons_log', 'number_of_episodes_log', 'vote_count_log', 'vote_average_log', 'episode_run_time']]\n",
    "y = df['popularity_log']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test data\n",
    "rf_predictions_test = rf.predict(X_test)\n",
    "\n",
    "# Predictions on train data\n",
    "rf_predictions_train = rf.predict(X_train)\n",
    "\n",
    "# Calculate evaluation metrics for test set\n",
    "test_mae = mean_absolute_error(y_test, rf_predictions_test)\n",
    "test_mse = mean_squared_error(y_test, rf_predictions_test)\n",
    "test_r2 = r2_score(y_test, rf_predictions_test)\n",
    "\n",
    "# Calculate evaluation metrics for train set\n",
    "train_mae = mean_absolute_error(y_train, rf_predictions_train)\n",
    "train_mse = mean_squared_error(y_train, rf_predictions_train)\n",
    "train_r2 = r2_score(y_train, rf_predictions_train)\n",
    "\n",
    "print(\"Results with Log Transformation, 4 features:\")\n",
    "# Print Train set evaluation metrics\n",
    "print(f'Train MAE: {train_mae:.4f}')\n",
    "print(f'Train MSE: {train_mse:.4f}')\n",
    "print(f'Train R^2: {train_r2:.4f}')\n",
    "\n",
    "# Print Test set evaluation metrics\n",
    "print(f'Test MAE: {test_mae:.4f}')\n",
    "print(f'Test MSE: {test_mse:.4f}')\n",
    "print(f'Test R^2: {test_r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with Genres Features:\n",
      "Train MAE: 0.8937\n",
      "Train MSE: 3.7112\n",
      "Train R^2: 0.8799\n",
      "Test MAE: 1.6614\n",
      "Test MSE: 11.7101\n",
      "Test R^2: 0.6197\n"
     ]
    }
   ],
   "source": [
    "X = df[['number_of_seasons', 'number_of_episodes', 'vote_count', 'vote_average', \n",
    "        'episode_run_time', 'Action & Adventure', 'Animation', 'Comedy', 'Crime', \n",
    "        'Documentary', 'Drama', 'Family', 'History', 'Kids', 'Music', 'Musical', \n",
    "        'Mystery', 'News', 'Reality', 'Romance', 'Sci-Fi & Fantasy', 'Soap', \n",
    "        'Talk', 'Unknown', 'War & Politics', 'Western']]\n",
    "y = df['popularity']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test data\n",
    "rf_predictions_test = rf.predict(X_test)\n",
    "\n",
    "# Predictions on train data\n",
    "rf_predictions_train = rf.predict(X_train)\n",
    "\n",
    "# Calculate evaluation metrics for test set\n",
    "test_mae = mean_absolute_error(y_test, rf_predictions_test)\n",
    "test_mse = mean_squared_error(y_test, rf_predictions_test)\n",
    "test_r2 = r2_score(y_test, rf_predictions_test)\n",
    "\n",
    "# Calculate evaluation metrics for train set\n",
    "train_mae = mean_absolute_error(y_train, rf_predictions_train)\n",
    "train_mse = mean_squared_error(y_train, rf_predictions_train)\n",
    "train_r2 = r2_score(y_train, rf_predictions_train)\n",
    "\n",
    "print(\"Results with Genres Features:\")\n",
    "# Print Train set evaluation metrics\n",
    "print(f'Train MAE: {train_mae:.4f}')\n",
    "print(f'Train MSE: {train_mse:.4f}')\n",
    "print(f'Train R^2: {train_r2:.4f}')\n",
    "\n",
    "# Print Test set evaluation metrics\n",
    "print(f'Test MAE: {test_mae:.4f}')\n",
    "print(f'Test MSE: {test_mse:.4f}')\n",
    "print(f'Test R^2: {test_r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with One-Hot Encoded Features:\n",
      "Train MAE: 0.1208\n",
      "Train MSE: 0.0401\n",
      "Train R^2: 0.9348\n",
      "Test MAE: 0.2751\n",
      "Test MSE: 0.1962\n",
      "Test R^2: 0.6800\n"
     ]
    }
   ],
   "source": [
    "X = df[['number_of_seasons_log', 'number_of_episodes_log', 'vote_count_log', \n",
    "        'vote_average_log', 'episode_run_time_log', 'Action & Adventure', \n",
    "        'Animation', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', \n",
    "        'History', 'Kids', 'Music', 'Musical', 'Mystery', 'News', 'Reality', \n",
    "        'Romance', 'Sci-Fi & Fantasy', 'Soap', 'Talk', 'Unknown', 'War & Politics', \n",
    "        'Western', 'original-language_en', 'original-language_zh', 'original-language_ja', \n",
    "        'original-language_ko', 'original-language_de', 'original-language_fr', \n",
    "        'original-language_es', 'original-language_pt', 'original-language_ru', \n",
    "        'original-language_nl', 'original-language_ar', 'original-language_da', \n",
    "        'original-language_cn', 'original-language_th', 'original-language_tr', \n",
    "        'original-language_it', 'original-language_hi', 'original-language_sv', \n",
    "        'original-language_cs', 'original-language_no', 'created-by_Shotaro Ishinomori', \n",
    "        'created-by_John de Mol', 'created-by_Adrián Suar', 'created-by_Simon Fuller', \n",
    "        'created-by_Ekta Kapoor', 'created-by_Na Young-seok', 'created-by_Yang Li-Hua', \n",
    "        'created-by_Joseph Barbera, William Hanna', 'created-by_R.J. Nuevas', \n",
    "        'created-by_Mark Burnett', 'networks_BBC One', 'networks_YouTube', \n",
    "        'networks_Netflix', 'networks_ITV1', 'networks_BBC Two', 'networks_ABC', \n",
    "        'networks_NBC', 'networks_TVB Jade', 'networks_CBS', 'networks_Channel 4', \n",
    "        'networks_ZDF', 'origin-country_US', 'origin-country_JP', 'origin-country_GB', \n",
    "        'origin-country_CN', 'origin-country_DE', 'origin-country_KR', 'origin-country_CA', \n",
    "        'origin-country_FR', 'origin-country_AU', 'origin-country_BR', 'origin-country_NL', \n",
    "        'origin-country_RU', 'origin-country_ES', 'origin-country_TH', 'origin-country_HK', \n",
    "        'origin-country_IN', 'origin-country_DK', 'origin-country_PH', 'origin-country_IT', \n",
    "        'origin-country_TR', 'origin-country_SE', 'origin-country_NO', 'origin-country_TW', \n",
    "        'origin-country_BE', 'origin-country_CZ', 'origin-country_MX', 'production-companies_US', \n",
    "        'production-companies_JP', 'production-companies_GB', 'production-companies_CN', \n",
    "        'production-companies_DE', 'production-companies_KR', 'production-companies_CA', \n",
    "        'production-companies_FR', 'production-companies_AU', 'production-companies_BR', \n",
    "        'production-companies_NL', 'production-companies_RU', 'production-companies_ES', \n",
    "        'production-companies_TH', 'production-companies_HK', 'production-companies_IN', \n",
    "        'production-companies_DK', 'production-companies_PH', 'production-companies_IT', \n",
    "        'production-companies_TR', 'production-companies_SE', 'production-companies_NO', \n",
    "        'production-companies_TW', 'production-companies_BE', 'production-companies_CZ', \n",
    "        'production-companies_MX']]\n",
    "y = df['popularity_log']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test data\n",
    "rf_predictions_test = rf.predict(X_test)\n",
    "\n",
    "# Predictions on train data\n",
    "rf_predictions_train = rf.predict(X_train)\n",
    "\n",
    "# Calculate evaluation metrics for test set\n",
    "test_mae = mean_absolute_error(y_test, rf_predictions_test)\n",
    "test_mse = mean_squared_error(y_test, rf_predictions_test)\n",
    "test_r2 = r2_score(y_test, rf_predictions_test)\n",
    "\n",
    "# Calculate evaluation metrics for train set\n",
    "train_mae = mean_absolute_error(y_train, rf_predictions_train)\n",
    "train_mse = mean_squared_error(y_train, rf_predictions_train)\n",
    "train_r2 = r2_score(y_train, rf_predictions_train)\n",
    "\n",
    "print(\"Results with One-Hot Encoded Features:\")\n",
    "# Print Train set evaluation metrics\n",
    "print(f'Train MAE: {train_mae:.4f}')\n",
    "print(f'Train MSE: {train_mse:.4f}')\n",
    "print(f'Train R^2: {train_r2:.4f}')\n",
    "\n",
    "# Print Test set evaluation metrics\n",
    "print(f'Test MAE: {test_mae:.4f}')\n",
    "print(f'Test MSE: {test_mse:.4f}')\n",
    "print(f'Test R^2: {test_r2:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
